\documentclass[]{report}
\usepackage{tikz}
\newcommand{\inputtikz}[2]{%  
	\scalebox{#1}{\input{#2}}  
}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{listings}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{caption}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
%opening

\begin{document}
	
\begin{titlepage}
	
	\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} 
	
	\center 
	
	\textsc{\LARGE Universit√© de Technologie de Compiegne}\\[1.5cm]
	\textsc{\Large SY19}\\[0.5cm] 
	\textsc{\large Machine Learning}\\[0.5cm]
		
	\HRule \\[0.4cm]
	{ \huge \bfseries Second Assignment}\\[0.4cm] 
	\HRule \\[1.5cm]
		
	\begin{minipage}{0.4\textwidth}
		\begin{flushleft} \large
			Aladin \textsc{TALEB} 
		\end{flushleft}
	\end{minipage}
	~
	\begin{minipage}{0.4\textwidth}
		\begin{flushright} \large
			Zineb \textsc{SLAM} 
		\end{flushright}
	\end{minipage}\\[2cm]

	{\large \today}\\[2cm] 

	\includegraphics[width=40mm]{Figures/utc.jpg}\\ % 

	\vfill
	
\end{titlepage}

\lstset{frame=tb,
	language=R,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	framexleftmargin=5mm,
	columns= fixed,
	numbers = left,
	basicstyle={\small\ttfamily},	
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}

	

\begin{abstract}
	

\end{abstract}


\tableofcontents



\section{Context}
This exercice aims to build the best classifier to recognize facial expressions based on a normalized photo of a given subject. The model will take advantage of a database of about 200 photos to learn how to classify the six different expressions : happiness, surprise, sadness, disgust, anger and fear.

\section{Dataset Description}
The very first step of our method consists in taking a look at the raw dataset. The dataset comprises 216 black and white photos of size 60 by 70, thus each case is described through 4200 features. The features are digits of the face feature and the responses describe the facial expression. There are 6 kind of reponses 1 to 6. Below an example of each face expression:

\begin{center}
	\includegraphics[width=0.8\linewidth]{Figures/happy_X32_y1.png}
	\captionof{figure}{y=1 Happy}
	\label{fig:y=1}
\end{center}

\begin{center}
	\includegraphics[width=0.8\linewidth]{Figures/shocked_X8_y2.png}
	\captionof{figure}{y=2 Shocked}
	\label{fig:y=2}
\end{center}

\begin{center}
	\includegraphics[width=0.8\linewidth]{Figures/cry_X132_y3.png}
	\captionof{figure}{y=3 Crying}
	\label{fig:y=3}
\end{center}

\begin{center}
	\includegraphics[width=0.8\linewidth]{Figures/mad_X54_y4.png}
	\captionof{figure}{y=4 Mad}
	\label{fig:}
\end{center}

\begin{center}
	\includegraphics[width=0.8\linewidth]{Figures/angry_X10_y5.png}
	\captionof{figure}{y=5 Angry}
	\label{fig:y=6}
\end{center}

\begin{center}
	\includegraphics[width=0.8\linewidth]{Figures/sad_X204_y6.png}
	\captionof{figure}{y=6 Sad}
	\label{fig:y=6}
\end{center}

 It turns out that a non-negligible part of those features are null, hence should be removed from the dataset.

The following code removes the null features from the dataset :
\begin{lstlisting}
X.clean= X[,colSums(X) != 0]
\end{lstlisting}
Each photo is now described through 3660 non-null features.
\pagebreak

\section{Classification}

\subsection{K Nearest Neighbors}
First we separate data between a train and a test data frame.

\begin{lstlisting}
set.seed (1)
train = sample(1:n, round(2*n/3))

train.X= X.clean[train, ]
train.Y= as.matrix(y[train, ])

test.X= X.clean[-train, ]
test.Y= as.matrix(y[-train, ])

train_set= as.data.frame(cbind(train.Y, train.X))
test_set= as.data.frame(cbind(test.Y, test.X))
train_set= data.frame(train.X, y= as.factor(train.Y))
test_set= data.frame(test.X, y= as.factor(test.Y))
\end{lstlisting}

We can start our study with a very simple model like the KNN to have an idea about the error. To perform a multi-classification in KNN we choose the k closest neighbor to each observation then each neighbor votes for a  class. At the end we pick the class that has the highest votes. 
First we perform a cross validation to pick the best k number of neighbors.

\subsubsection{Model Implementation}
To perform a CV on KNN we use the function \textbf{kknn.train} defined in the library \textbf{kknn}. As we do not have that much observation we can afford the computation of the Leave One Out.
\begin{lstlisting}
model.kknn= train.kknn(y~., data= train_set, kmax = 30)
model.kknn.best.k = model.kknn$best.parameters$k
\end{lstlisting}
We find 28 to be the number of neighbors that minimizes the train error. Now we use this k on our knn model.

\begin{lstlisting}
model.knn.pred= knn(train.X, test.X, train.Y ,k=model.kknn$best.parameters$k)
\end{lstlisting}

The knn gives out the index of the 28 closest points to each observation. We then get the class of each of those neighbors using the function\textbf{ attr}. We count the cardinal of each class and pick the highest one to be the response of the observation.
\begin{lstlisting}
n_test = nrow(test.X)
#Vector with prediction of the class of each test observation
knn_pred = matrix(, nrow=n_test , ncol=1) 

for(i in 1:n_test)
{

    #Class of each neighbor of the observation i
    iline = train_set$y[attr(model.knn.pred, 'nn.index')[i,]]

    # Count votes for each class
    count1 = length(which(iline[] == 1)) 
    count2 = length(which(iline[] == 2)) 
    count3 = length(which(iline[] == 3)) 
    count4 = length(which(iline[] == 4)) 
    count5 = length(which(iline[] == 5)) 
    count6 = length(which(iline[] == 6)) 

    #Vector of votes
    votes       = c(count1, count2, count3, count4, count5, count6)

    #I need to select the class with the highest number of neighbors
    max_vote = which.max(votes)

    knn_pred[i] = max_vote
}
\end{lstlisting}
The vector knn\_pred has the response of each test observation. We construct the confusion matrix by comparing the true responses to the predicted ones. The values that are not n the diagonal are the miss-classified ones. We also compute the error.

\begin{lstlisting}
confusion_matrix_knn = table(predict=knn_pred , truth= test_set$y)
errors_knn = sum(knn_pred!=test_set$y)/length(test_set$y)
# 26.3% errors
\end{lstlisting}

\subsubsection{Model Analysis}
The results of the KNN are shown below.
\begin{verbatim}
      truth
predict  1  2  3  4  5  6
      1 11  0  0  0  0  2
      2  0 15  0  0  0  1
      3  0  0  9  0  1  1
      4  1  1  1  8  2  3
      5  0  0  2  1  8  1
      6  2  0  0  0  0  2
0.2638889
\end{verbatim}
We notice that 19 observations were miss-classified, we thus get 26\% of errors. Considering than the KNN is a very simple non parametric approach we can say that this is not a bad estimation. Therefore we will use more complex models in the next parts to minimize the test error. 26\% is out highest bound of the test error.

\pagebreak

\subsection{LDA - Linear Discriminant Analysis}
The first classifier we are going to use is built using the Linear Discriminant Analysis method, based on Bayes' Theorem. This simple and straight-forward model allows us to have a quick idea of the performance we can expect from more complex methods.  

\subsubsection{Method Implementation}
The function \texttt{lda} is available to fit the model onto a given dataset. It returns the group means that are the average of each predictor in each class. The coefficients of linear discriminants output are used to form the LDA decision rule. The prior probability is the percentage of the response for each class in the observation.

\begin{lstlisting}
library(MASS)
model.lda = lda(y ~ ., data = train_set)
summary(model.lda)
\end{lstlisting}

Due to the relatively high number of features, it takes a couple of seconds for the algorithm to fit the model.  Once the model is trained, we call the \texttt{predict} function to predict the responses of the test set : 

\begin{lstlisting}
model.lda.predicted = predict(model.lda, newdata = test_set)
perfMeasure(model.lda.predicted$class, test_set.y)
\end{lstlisting}

\begin{verbatim}
    class
pred  1  2  3  4  5  6
1 11  0  0  0  0  5
2  0  9  0  1  0  0
3  0  0  9  0  1  2
4  0  0  0 16  0  0
5  0  0  0  1  8  3
6  0  0  0  0  0  8

0.1756757
\end{verbatim}

\subsubsection{Data visualisation using FDA}
One of the biggest problems of high-dimensional spaces is the unability to easiliy plot it in order to study and to evaluate the boundaries between each class.

The FDA method allows us to greatly reduce the number of dimensions. More precisely, let $K$ the number of classes, FDA tries to describe the entire dataset using $K-1$ components. In our case, $K = 6$, therefore we can easily reduce the number of dimensions from 3660 down to only 5, which is way easier to plot.

\begin{lstlisting}
% APPLY FDA TRANSFORMATION
U = model.lda$scaling
Z = train_set.X %*% U

dim1 = 1
dim2 = 2

% PLOT EACH SAMPLE IN THE FDA SPACE
plot(Z[train_set.y==1,dim1],Z[train_set.y==1,dim2], xlim=range(Z[,dim1]),ylim=range(Z[,dim2]))
points(Z[train_set.y==2,dim1],Z[train_set.y==2,dim2],pch=2,col=2)
points(Z[train_set.y==3,dim1],Z[train_set.y==3,dim2],pch=3,col=3)
points(Z[train_set.y==4,dim1],Z[train_set.y==4,dim2],pch=4,col=4)
points(Z[train_set.y==5,dim1],Z[train_set.y==5,dim2],pch=5,col=5)
points(Z[train_set.y==6,dim1],Z[train_set.y==6,dim2],pch=6,col=6)
\end{lstlisting}

\subsubsection{Dimension Reduction with PCA}
3660 is a quite high-number of features that slows down both fitting and prediction algorithms, and which may prevent the model from fitting properly. More specifically, the number of dimensions of this dataset is way higher than the number of samples, which means that the model may not be able to capture all relationships between the inputs and the response. One way of improving the performance of the algorithm, both in terms of accuracy and speed, is to decrease the number of inputs.

The FDA methods described previously is a very powerful dimension reduction technique that consists in changing the space using a linear transformation. However, LDA's accuracy score doesn't depends on space transformation, thus coupling FDA with LDA does not make any change.

Another way of reducing the number of features is using a Principal Component Analysis method. Such method aims to find the set of vectors, called principal components, that best describe the features variance. We can then select the best subset of principal components, which is the set of features that yields to the highest performance.

\begin{lstlisting}
fit_lda_pca = function() {
	model.lda.pca = lda(y ~ ., data = train_set.pca)
	model.lda.pca.predicted = predict(model.lda.pca, newdata = test_set.pca)
	p = perfMeasure(model.lda.pca.predicted$class, test_set.pca$y)
	return(p)
}

\end{lstlisting}

The function \texttt{prcomp} is available to compute the set of principal components using linear transformations. The same sequence of linear transformations should be applied on the test set as well, using the function \texttt{predict} : 
\begin{lstlisting}
pca = prcomp(train_set.X, center = TRUE, scale = TRUE)
nb_comp = 10 #Arbitrary Number

train_set.pca.X = as.data.frame(pca$x[,1:nb_comp])
train_set.pca = data.frame(train_set.pca.X)
train_set.pca["y"] = train_set.y

test_set.pca.X = predict(pca, newdata = test_set.X)[,1:nb_comp]
test_set.pca = data.frame(test_set.pca.X)
test_set.pca["y"] = test_set.y

fit_lda_pca()
\end{lstlisting}

The performance now mostly relies on the number of principal components used to describe the dataset. This parameter should be determined using a cross-validation technique as not to introduce any biais. Since fitting a LDA model does not take much time, we decided to use a 10-fold cross validaiton method between : 

\begin{lstlisting}
nb_folds = 10
accs = matrix(0, 100, 1)
accs[1] = Inf
for (M in 2:100) {
	printf("Nb PCA %d", M)
	a.train_set.pca.X = as.data.frame(pca$x[,1:M])
	a.train_set.pca = data.frame(a.train_set.pca.X)
	a.train_set.pca["y"] = train_set.y
	
	a.test_set.pca.X = predict(pca, newdata = test_set.X)[,1:M]
	a.test_set.pca = data.frame(a.test_set.pca.X)
	a.test_set.pca["y"] = test_set.y
	
	folds = createFolds(a.train_set.pca$y, k = nb_folds)
	
	acc = 0;
	for (k in 1:nb_folds) {
		validation_indexes = folds[[k]]
		train_set.pca.X = a.train_set.pca.X[-validation_indexes,]
		train_set.pca = a.train_set.pca[-validation_indexes,]
		
		test_set.pca.X = a.train_set.pca.X[validation_indexes,]
		test_set.pca = a.train_set.pca[validation_indexes,]
		
		acc = acc + fit_lda_pca()
	}
	
	acc = acc / nb_folds
	accs[M] = acc
}
min(accs)
which.min(accs)
\end{lstlisting}

It turns out that the 27 components yields to the lowest cross-validation error rate of about 14\%. We can now apply this model on the test set : the error-rate is about 16.2\%.



\pagebreak
\subsection{Support Vector Machines}
In this part we discuss another approach of classification : The support vector machine with the kernel option. We will use a support vector approach to predict the facial expression using face features expressions

\subsubsection{Model Implementation}
The \textbf{e1071} library includes the \textbf{tune()} function, to perform cross-validation (by default, it performs 10-fold cross-validation) to determine the best tuning parameter.
\begin{lstlisting}
model.tune= tune(svm, y~., data= svm.train, kernel= "linear", ranges= list(c(0.1, 1, 10), gamma= c(0.1, 1, 10)))
summary(model.tune)
\end{lstlisting}

We see that cost=0.1 results in the lowest cross-validation error rate. 
The tune() function stores the best model obtained, which can be accessed as follows: \textbf{model.tune\$best.parameters} We can now perform the SVM with those best tuning parameters.

The argument scale=TRUE tells the \textbf{svm()} function to scale each feature to have mean zero or standard deviation one; we thought it would be more...
This data set has a very large number of features compared to observations. This means that likely we could use a linear kernel. In Other words we want to compare SVMs with a linear kernel.

\begin{lstlisting}
model.svm= svm(y~., data=svm.train, kernel= 'linear', gamma= model.tune$best.parameters$gamma , cost= model.tune$best.parameters$Var1)
summary(model.svm) 
\end{lstlisting}
We see that the function identified 6 classes. This tells us, for instance, that a linear kernel was used with cost=... , 
and that there were ... support vectors, .. in class 1 and ... in class 2, ....

We can now plot the support vector classifier obtained:

\begin{lstlisting}
plot(x= model.svm, data= svm.train, formula= X1~X2)
\end{lstlisting}

We(Note that here the second feature is plotted on the x-axis and the first feature is plotted on the y-axis, 
The region that is assigned to each class is shown on the right (light blue for 1, ...). The decision boundary between the two classes is linear 
(because we used the argument kernel="linear"). 
We see that in this case only 6 observations are misclassified. 

Training errors = we only made 6 errors on the training data
\begin{lstlisting}
confusion_matrix_train= table(model.svm$fitted, svm.train$y)
train_errors= sum(model.svm$fitted != svm.train$y)/length(svm.train$y)
\end{lstlisting}

\subsubsection{Model Analysis}


\section{Conclusion}
\end{document}
