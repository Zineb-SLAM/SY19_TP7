	\documentclass[]{report}
\usepackage{tikz}
\newcommand{\inputtikz}[2]{%  
	\scalebox{#1}{\input{#2}}  
}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
%\usepackage[colorinlistoftodos]{todonotestfa}
\usepackage{listings}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{caption}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
%opening

\begin{document}
	
\begin{titlepage}
	
	\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} 
	
	\center 
	
	\textsc{\LARGE Université de Technologie de Compiegne}\\[1.5cm]
	\textsc{\Large SY19}\\[0.5cm] 
	\textsc{\large Machine Learning}\\[0.5cm]
		
	\HRule \\[0.4cm]
	{ \huge \bfseries Second Assignment}\\[0.4cm] 
	\HRule \\[1.5cm]
		
	\begin{minipage}{0.4\textwidth}
		\begin{flushleft} \large
			Aladin \textsc{TALEB} 
		\end{flushleft}
	\end{minipage}
	~
	\begin{minipage}{0.4\textwidth}
		\begin{flushright} \large
			Zineb \textsc{SLAM} 
		\end{flushright}
	\end{minipage}\\[2cm]

	{\large \today}\\[2cm] 

	\includegraphics[width=40mm]{Figures/utc.jpg}\\ % 

	\vfill
	
\end{titlepage}

\lstset{frame=tb,
	language=R,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	framexleftmargin=5mm,
	columns= fixed,
	numbers = left,
	basicstyle={\small\ttfamily},	
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}

	

\begin{abstract}

	This report is the second assignment for the course SY19 : Machine Learning. It is focused on a single exercise : a classification problem, on which we will describe the methodology we used to build the best model for the given dataset.\\
	The content presented in this report is the result of the theoretical and practical courses of SY19 taught by Thierry Denoeux and a literature review. We put on a side note that we both have not followed SY09, and only one of us has already done SY02, which prevented us to go deeper in the explanations and analysis sometimes but we made sure to make researches every time it was needed.
	
\end{abstract}

\tableofcontents

\chapter{Classification Exercice}



\section{Context}
This exercice aims to build the best classifier to recognize facial expressions based on a normalized photo of a given subject. The model will take advantage of a database of about 200 photos to learn how to classify the six different expressions : happiness, surprise, sadness, disgust, anger and fear.

\section{Dataset Description}
The very first step of our method consists in taking a look at the raw dataset. The dataset comprises 216 black and white photos of size 60 by 70, thus each case is described through 4200 features. Each feature represents a single pixel of the face, and the responses describe the facial expression. There are 6 kind of reponses : 1 to 6, each of which is described through 36 different photos. Below an example of each face expression:


\begin{center}
	\includegraphics[width=0.6\linewidth]{Figures/happy_X32_y1.png}
	\captionof{figure}{$y=1$ Happy}
	\label{fig:y=1}
\end{center}

\begin{center}
	\includegraphics[width=0.6\linewidth]{Figures/shocked_X8_y2.png}
	\captionof{figure}{$y=2$ Shocked}
	\label{fig:y=2}
\end{center}

\begin{center}
	\includegraphics[width=0.6\linewidth]{Figures/cry_X132_y3.png}
	\captionof{figure}{$y=3$ Disapointed}
	\label{fig:y=3}
\end{center}

\begin{center}
	\includegraphics[width=0.6\linewidth]{Figures/mad_X54_y4.png}
	\captionof{figure}{$y=4$ Mad}
	\label{fig:}
\end{center}

\begin{center}
	\includegraphics[width=0.6\linewidth]{Figures/angry_X10_y5.png}
	\captionof{figure}{$y=5$ Angry}
	\label{fig:y=6}
\end{center}

\begin{center}
	\includegraphics[width=0.6\linewidth]{Figures/sad_X204_y6.png}
	\captionof{figure}{$y=6$ Sad}
	\label{fig:y=6}
\end{center}

Each portrait is normalized so that the main features of the face are located more or less on the same areas. Therefore, our algorithms will not have to cope with any transformations : translation, rotation and scaling.

It turns out that a non-negligible part of those features are null. Any null features may prevent the following classifiers to fit properly, so we have to remove them as soon as possible. The following code removes the null features from the dataset :
\begin{lstlisting}
X.clean= X[,colSums(X) != 0]
\end{lstlisting}
Each photo is now described through 3660 non-null features. Nevertheless, 3660 is still a very high number of features which prevents us from properly visualizing the dataset. 

Moreover, the number of cases is very small compared to the number of features : how can only 200 cases be enough to understand how more than 3000 low-level features work together ? It already seems that the small dataset will prevent us from building accurate classifiers.

\section{Measure to Compare Models}
Before building any model, we have to properly define the measures we will use later to compare their performance. In a classification problem, "accuracy" is often used. It is defined by :
$$
A = \frac{\text{Number of Good Predictions}}{\text{Number of Cases}}
$$

This is a very simple measure that may cause some issues when applied on classification problems that deal with skewed classes. For instance, let $A$ and $B$, two classes. $A$ contains 95\% of the dataset, while $B$ contains the remaining 5\%. Let $M$ a pretty bad classifier which classifies the whole dataset in class $A$. According to the definition stated above, this classifier has an accuracy of 95\%, which is excellent. In this case, more specific performance measures have to be applied.

However, our current classification problem does not deal with skewed class since all classes are equally-represented in the dataset. Therefore, we will be able to run this performance measure without any important issue. We may also use the error-rate, which is equal to $1 - A$.

 These measures will be applied on the test set in order not to include any bias.

\begin{lstlisting}
set.seed (1)
train = sample(1:n, round(2*n/3))

train.X= X.clean[train, ]
train.Y= as.matrix(y[train, ])

test.X= X.clean[-train, ]
test.Y= as.matrix(y[-train, ])

train_set= as.data.frame(cbind(train.Y, train.X))
test_set= as.data.frame(cbind(test.Y, test.X))
train_set= data.frame(train.X, y= as.factor(train.Y))
test_set= data.frame(test.X, y= as.factor(test.Y))
\end{lstlisting}

\pagebreak
\section{Classification}

\subsection{K Nearest Neighbors}
We start our study with a very simple model like the KNN to have an idea about the Bayes error rate, which is the minimum achievable error rate. 

To perform a multi-classification in KNN we choose the $k$ closest neighbors to each observation then each neighbor votes for a  class. At the end we pick the class that has the highest votes. The parameter $k$ should be found using a cross-validation method, as not to introduce any biais.

\subsubsection{Model Implementation}
To perform a CV on KNN we use the function \texttt{kknn.train} defined in the library \texttt{kknn}. As we do not have that many observations, we can afford the computation of the Leave One Out.
\begin{lstlisting}
library("kknn")
model.kknn= train.kknn(y~., data= train_set, kmax = 30)
model.kknn.best.k = model.kknn$best.parameters$k
\end{lstlisting}
We find 28 to be the number of neighbors that minimizes the train error. Now we use this $k$ on our KNN model.

\begin{lstlisting}
library('FNN')
model.knn.pred= knn(train.X, test.X, train.Y ,k=model.kknn$best.parameters$k)
\end{lstlisting}

The KNN gives out the index of the 28 closest points to each observation. We then get the class of each of those neighbors using the function \texttt{attr}. We count the cardinal of each class and pick the highest one to be the response of the observation.
\pagebreak

\begin{lstlisting}
n_test = nrow(test.X)
#Vector with prediction of the class of each test observation
knn_pred = matrix(, nrow=n_test , ncol=1) 

for(i in 1:n_test)
{

    #Class of each neighbor of the observation i
    iline = train_set$y[attr(model.knn.pred, 'nn.index')[i,]]
    # Count votes for each class
    count1 = length(which(iline[] == 1)) 
    count2 = length(which(iline[] == 2)) 
    count3 = length(which(iline[] == 3)) 
    count4 = length(which(iline[] == 4)) 
    count5 = length(which(iline[] == 5)) 
    count6 = length(which(iline[] == 6)) 
    #Vector of votes
    votes       = c(count1, count2, count3, count4, count5, count6)
    #I need to select the class with the highest number of neighbors
    max_vote = which.max(votes)
    knn_pred[i] = max_vote
}
\end{lstlisting}
The vector \texttt{knn\_pred} has the predicted response of each test observation. We construct the confusion matrix by comparing the true responses to the predicted ones. The values that are not in the diagonal are the miss-classified ones. We can then compute the error-rate according to the formula described above.
\begin{lstlisting}
confusion_matrix_knn = table(predict=knn_pred , truth= test_set$y)
errors_knn = sum(knn_pred!=test_set$y)/length(test_set$y)
# 26.3% errors
\end{lstlisting}
\pagebreak
\subsubsection{Model Analysis}
The results of the KNN are shown below.
\begin{verbatim}
      truth
predict  1  2  3  4  5  6
      1 	11  0  0  0  0  2
      2  	0 15  0  0  0  1
      3  	0  0  9  0  1  1
      4  	1  1  1  8  2  3
      5  	0  0  2  1  8  1
      6  	2  0  0  0  0  2
0.2638889
\end{verbatim}
We notice that 19 observations out of 72 were miss-classified, we thus get 26\% of errors. Considering than the KNN is a very simple non parametric approach we can say that this is not a bad estimation. From now on, we will use more complex models in the next parts to minimize the test error. 

\pagebreak

\subsection{LDA - Linear Discriminant Analysis}
The next classifier we are going to use is built using the Linear Discriminant Analysis (LDA) method, based on Bayes' Theorem. This simple and straight-forward model allows us to have a quick idea of the performance we can expect from more complex methods.  

\subsubsection{Method Implementation}
The function \texttt{lda} is available to fit the model onto a given dataset. It returns the group means that are the average of each predictor in each class. The coefficients of linear discriminants output are used to form the LDA decision rule. The prior probability is the percentage of the response for each class in the observation.

\begin{lstlisting}
library(MASS)
model.lda = lda(y ~ ., data = train_set)
summary(model.lda)
\end{lstlisting}

Due to the relatively high number of features, it takes a couple of seconds for the algorithm to fit the model.  Once the model is trained, we call the \texttt{predict} function to predict the responses of the test set : 

\begin{lstlisting}
model.lda.predicted = predict(model.lda, newdata = test_set)
perfMeasure(model.lda.predicted$class, test_set.y)
\end{lstlisting}

\begin{verbatim}
    class
p  1  2  3  4  5  6
1  9  0  0  0  0  1
2  0 14  0  0  0  0
3  0  0 10  0  1  1
4  0  1  0  9  2  0
5  0  1  2  0  8  1
6  5  0  0  0  0  7

0.2083333
\end{verbatim}

This very simple model yields to a decent error-rate of about 20.8\%

\subsubsection{Data visualisation using FDA}
One of the biggest problems of high-dimensional spaces is the unability to easiliy plot it in order to study and to evaluate the boundaries that separate each class.

The FDA method allows us to greatly reduce the number of dimensions. More precisely, let $K$ the number of classes, FDA tries to describe the entire dataset using $K-1$ components. In our case, $K = 6$, therefore we can easily reduce the number of dimensions from 3660 down to only 5, which is way easier to plot.

\begin{lstlisting}
# APPLY FDA TRANSFORMATION
U = model.lda$scaling
Z = train_set.X %*% U

dim1 = 1
dim2 = 2

# PLOT EACH SAMPLE IN THE FDA SPACE
plot(Z[train_set.y==1,dim1],Z[train_set.y==1,dim2], xlim=range(Z[,dim1]),ylim=range(Z[,dim2]))
points(Z[train_set.y==2,dim1],Z[train_set.y==2,dim2],pch=2,col=2)
points(Z[train_set.y==3,dim1],Z[train_set.y==3,dim2],pch=3,col=3)
points(Z[train_set.y==4,dim1],Z[train_set.y==4,dim2],pch=4,col=4)
points(Z[train_set.y==5,dim1],Z[train_set.y==5,dim2],pch=5,col=5)
points(Z[train_set.y==6,dim1],Z[train_set.y==6,dim2],pch=6,col=6)
\end{lstlisting}

\begin{center}
	\includegraphics[width=0.6\linewidth]{Figures/plot_fda_1_2.png}
	\captionof{figure}{Plot of the training set using a projection onto the first two components of the FDA space.}
	\label{fig:plot_fda_1_2}
\end{center}

It turns out that the dataset seems to be fairly separable in the space defined by the FDA components which mean that this dimension reduction may yield to interesting performance results. 

\subsubsection{Dimension Reduction with PCA}
3660 is a quite high-number of features that slows down both fitting and prediction algorithms, and which may prevent the model from fitting properly. More specifically, the number of dimensions of this dataset is way higher than the number of samples, which means that the model may not be able to capture all relationships between the inputs and the response. One way of improving the performance of the algorithm, both in terms of accuracy and speed, is to decrease the number of inputs.

The FDA methods described previously is a very powerful dimension reduction technique that consists in changing the space using a linear transformation. However, LDA's accuracy score does not depend on space transformation, thus coupling FDA with LDA would not make any change.

Another way of reducing the number of features is using a Principal Component Analysis method. Such method aims to find the set of vectors, called principal components, that best describe the features variance. We can then select the best subset of principal components, which is the set of features that yields to the highest performance.

\begin{lstlisting}
fit_lda_pca = function() {
	model.lda.pca = lda(y ~ ., data = train_set.pca)
	model.lda.pca.predicted = predict(model.lda.pca, newdata = test_set.pca)
	p = perfMeasure(model.lda.pca.predicted$class, test_set.pca$y)
	return(p)
}

\end{lstlisting}

The function \texttt{prcomp} is available to compute the set of principal components using linear transformations. The same sequence of linear transformations should be applied on the test set as well, using the function \texttt{predict} : 
\begin{lstlisting}
pca = prcomp(train_set.X)
nb_comp = 10 #Arbitrary Number

train_set.pca.X = as.data.frame(pca$x[,1:nb_comp])
train_set.pca = data.frame(train_set.pca.X)
train_set.pca["y"] = train_set.y

test_set.pca.X = predict(pca, newdata = test_set.X)[,1:nb_comp]
test_set.pca = data.frame(test_set.pca.X)
test_set.pca["y"] = test_set.y

fit_lda_pca()
\end{lstlisting}

The performance now mostly relies on the number of principal components used to describe the dataset. This parameter should be determined using a cross-validation technique as not to introduce any biais. Since fitting a LDA model does not take much time, we decided to use a 10-fold cross validaiton method between : 

\begin{lstlisting}
nb_folds = 10
accs = matrix(0, 100, 1)
accs[1] = Inf
for (M in 2:100) {
	printf("Nb PCA %d", M)
	a.train_set.pca.X = as.data.frame(pca$x[,1:M])
	a.train_set.pca = data.frame(a.train_set.pca.X)
	a.train_set.pca["y"] = train_set.y
	
	a.test_set.pca.X = predict(pca, newdata = test_set.X)[,1:M]
	a.test_set.pca = data.frame(a.test_set.pca.X)
	a.test_set.pca["y"] = test_set.y
	
	folds = createFolds(a.train_set.pca$y, k = nb_folds)
	
	acc = 0;
	for (k in 1:nb_folds) {
		validation_indexes = folds[[k]]
		train_set.pca.X = a.train_set.pca.X[-validation_indexes,]
		train_set.pca = a.train_set.pca[-validation_indexes,]
		
		test_set.pca.X = a.train_set.pca.X[validation_indexes,]
		test_set.pca = a.train_set.pca[validation_indexes,]
		
		acc = acc + fit_lda_pca()
	}
	
	acc = acc / nb_folds
	accs[M] = acc
}
min(accs)
which.min(accs)
\end{lstlisting}

We ran this algorithm many times and we found different results depending on the cross-validation split : 

\begin{center}
	\includegraphics[width=0.6\linewidth]{Figures/lda_pca_cv.png}
	\captionof{figure}{10 fold cross validation to find the best number of principal components to use for LDA.}
	\label{fig:lda_pca_cv}
\end{center}

It turns out that about 35 to 35 principal components yields to the lowest cross-validation error rate of about 14\%. We can now apply this model on the test set : we found an error-rate of roughly 20\%.

\begin{verbatim}
    class
p  1  2  3  4  5  6
1 11  0  0  0  0  1
2  0 14  0  0  0  0
3  0  0 11  0  1  2
4  0  1  0  9  3  0
5  0  1  1  0  7  0
6  3  0  0  0  0  7

0.1805556
\end{verbatim}

\pagebreak

\subsection{Neural Networks}

Neural Networks are a set of methods based on a collection of computation units called artificial neurons, each of which is basically capable of extracting a new features based on a linear combination of its inputs. The training algorithms mainly consist in tweaking each neuron so that it extracts the feature that minimize the error-rate of the whole network.

These classifiers are very powerful, but this power comes to a price of : 
\begin{itemize}
	\item a very long training time
	\item a non-convex error function to optimize, which means the optimization algorithm may reach a local optima
	\item a high number of parameters one should tweak in order to come up with the best network for a given dataset
\end{itemize}
To simplify our analysis, we will only deal with single hidden layer perceptrons that uses sigmoid neurons. 

In R, the library \texttt{nnet} is available to build simple neural network classifiers and regression models : 
\begin{lstlisting}
library(nnet)

train_set$y = factor(train_set$y)
test_set$y = factor(test_set$y)

model.nnet = nnet(y ~ ., data=train_set, size=5, MaxNWts = 20000)
model.nnet.predicted = predict(model.nnet, test_set, type="class")
perfMeasure(model.nnet.predicted, test_set$y)
\end{lstlisting}

\subsubsection{Dimension Issue}

However, in our case, the number of features is way too high for a casual computer. Indeed, a single layer perceptron is described through $W$ parameters which represent the neurons' weights : 

$$
	W = M(P+1) + K(M+1)
$$

with $M$ the number of neurons, $P$ the number of features and $K$ the number of classes. In our case, $W \approx 3660M$.
 therefore, the only parameter we have to tweak is the number of neurons contained in the hidden layer. This number should be determined using a cross-validation technique. However, even though $W$ grows in a linear way, it increases too fast for the fitting algorithm to perform in a timely manner. 
 
 We were able to fit a neural network with only 2 and 5 hidden neurons, it took respectively less than a minute and about 10 minutes for our laptop to train the model. They both yield to miserable error rates of 85\% and 50\%. The main problem here is the ridiculously low number of neurons, a good rule of thumb being choosing $M$ between $K$ and $P$.
 
The main way to carry the analysis further would be to first apply a dimension reduction algorithm on the inputs.

\subsubsection{Dimension Reduction}
One such reduction algorithm is the FDA that we have already described previously. We now only have to cope with 5 different inputs, which is way easier for the optimization algorithm to work with. 

A 5-fold cross-validation algorithm can be performed in a timely manner. $M = 95$ yields to a very low error-rate of 5\% on the validaiton set, but a quite high error-rate of 27\% on the test set. This difference in the error-rate comes from the fact that a single hidden layer tend to overfit with a high number of neurons.

\begin{center}
	\includegraphics[width=0.6\linewidth]{Figures/nnet_fda_cv.png}
	\captionof{figure}{5 fold cross validation to find the best number of hidden neurons.}
	\label{fig:nnet_fda_cv}
\end{center}

\begin{verbatim}
    class
p  1  2  3  4  5  6
1 10  0  0  0  0  0
2  0 13  0  0  0  0
3  0  0 10  0  1  1
4  1  1  2  7  5  0
5  0  1  0  2  4  1
6  3  1  0  0  1  8

0.2777778
\end{verbatim}

We could also take advantage of a Principal Component Analysis method to decrease the dimension. However, this would mean performing a nested cross-validation method in order to find the best number of components that goes along with the best number of neurons. This would definitely take too much time to train, for a result that may not be as interesting as we think.

\subsubsection{Regularization}
 It is possible to prevent the neural network from overfitting by using a regularization technique that basically constraints each neuron's weights using a parameter $\lambda$. The main idea is now to use a high number of neurons, before finding the best regularization parameter  $\lambda$ by cross-validation.
 
 We first apply this method on the dataset whose dimensions have been reduced using the FDA technique. Using $M = 50$, a fairly high number compared to the number of inputs and outputs, the best $\lambda$ seems to be 2, which yields to an error-rate ranged from 20\% to 24\% depending on the optimization results.
 
 \begin{center}
 	\includegraphics[width=0.6\linewidth]{Figures/nnet_fda_reg_cv.png}
 	\captionof{figure}{10 fold cross validation to find the best regularization term $\lambda$.}
 	\label{fig:nnet_fda_reg_cv}
 \end{center}

\begin{verbatim}
    class
p  1  2  3  4  5  6
1 13  0  0  0  0  1
2  0 13  1  0  0  0
3  0  0  8  0  1  1
4  0  2  0  9  3  0
5  0  1  2  0  7  1
6  1  0  1  0  0  7

0.2083333
\end{verbatim}

\subsubsection{Conclusion}
Neural Networks are very powerful models than can get as close of a given function as needed. However, it needs a fairly high number of samples to be interesting enough for our needs. Unfortunately, the high number of features prevents us from working properly since each training can take up to about 10 minutes. A way of speeding things up is to use convolution networks on a computer with enough GPU power to handle the training phase.

\pagebreak

\subsection{Decision Trees}
 In this section, we will discuss a set of machine learning methods called decision trees. A decision tree is a very intuitive way to build a classifier (or a regression model) that aims to split the space into smaller pieces, a simple condition is then defined on a single parameter to guide the decision throughout the decision space. A classification tree is often used when an understable and interpretable classifier is needed, however, in our case, we do not need such property.
 
 A single tree can be grown using the function \texttt{tree} available in the library of the same name : 
 \begin{lstlisting}
 library(tree)
 
 model.tree = tree(as.factor(y) ~ ., train_set) 
 model.tree.predicted = predict(model.tree, test_set, type="class")
 \end{lstlisting}
 
  \begin{center}
 	\includegraphics[width=0.6\linewidth]{Figures/tree_full.png}
 	\captionof{figure}{Decision Tree grown on the original dataset.}
 	\label{fig:tree_full}
 \end{center}

\begin{verbatim}
    class
p  1  2  3  4  5  6
1 10  0  0  0  0  1
2  0 11  0  0  0  0
3  0  1  6  1  2  2
4  0  1  2  3  2  1
5  1  0  3  3  5  1
6  3  3  1  2  2  5

0.4444444
\end{verbatim}
 
 The grown tree has only 12 terminal nodes. The error-rate on the train set is less than 10\%, however the error-rate on the test set is over 40\%, which means this tree tends to overfit the train set. We could slightly improve the performance of the tree by pruning it using a cross validation method, but this would not make much sense for a 3660-dimension space made of very low-level features : how could a handful of conditions on pixel values could be enough to determine the correct emotion in most cases ?
 
 That is why we can once again reduce the number of dimensions using FDA. The tree has a 7 terminal nodes, a 10 fold cross-validation algorithm confirms that there is no need for pruning this tree. This decision tree yields to an error-rate of about 20\%, which is better but not good enough to stand against other methods.
 
   \begin{center}
 	\includegraphics[width=0.6\linewidth]{Figures/tree_fda.png}
 	\captionof{figure}{Decision Tree grown on the FDA set.}
 	\label{fig:tree_fda}
 \end{center}
 
 \begin{verbatim}
     class
 p  1  2  3  4  5  6
 1 12  0  0  0  0  1
 2  0 10  0  0  0  0
 3  0  1  8  0  1  1
 4  0  1  0  9  3  0
 5  0  4  4  0  7  1
 6  2  0  0  0  0  7

0.2638889
 \end{verbatim}
 
    \begin{center}
 	\includegraphics[width=0.6\linewidth]{Figures/tree_fda_cv.png}
 	\captionof{figure}{Cross-Validation to find the best size ot the Decrision Tree grown on the FDA set.}
 	\label{fig:tree_fda_cv}
 \end{center}
 
 It is possible to improve the accuracy of such methods by combining multiple decision trees, the main tradeoff being the unability to interpret the final classifier. This technique consists in building $B$ classification trees using slightly different datasets, getting the response related to an input can be done by having each tree vote for a single class.
 
\begin{lstlisting}
library(randomForest))

model.random_forest =randomForest(as.factor(y) ~ ., data=train_set)
model.random_forest.predicted = predict(model.random_forest,newdata=test_set,type='response')
\end{lstlisting}

\begin{verbatim}
# Raw dataset

    class
p  1  2  3  4  5  6
1  9  0  0  0  0  1
2  0 15  0  0  0  0
3  0  0 11  0  3  2
4  0  1  0  8  4  0
5  0  0  1  1  4  0
6  5  0  0  0  0  7

0.25
\end{verbatim}

\begin{verbatim}
# FDA dataset

    class
p  1  2  3  4  5  6
1 12  0  0  0  0  1
2  0 14  0  0  0  0
3  0  0  9  0  0  1
4  0  1  0  9  2  0
5  0  1  3  0  9  1
6  2  0  0  0  0  7

0.1666667
\end{verbatim}

We obtain an error-rate of 27\% with the raw dataset, which is better than using a single tree. The error-rate on the FDA dataset is now 16.6\%.
 
\pagebreak
\subsection{Support Vector Machines}
In this part, we discuss another approach of classification : The Support Vector Machine (SVM) with the kernel option. A SVM model tries to find the hyperplan than best separates two classes, defined as the hyperplan that enables each point to be as far as possible from it. If the dataset is not linearly separable, which is our case, SVM relies on a tuning parameter $C$ that control how loose the hyperplan should separate the dataset.

The kernel trick allows the boundaries to be non-linear. However, since our data set has a very large number of features compared to the number of observations $P \gg N$, we should stick with the linear kernel. 

\subsubsection{Model Implementation}
The \texttt{e1071} library includes the \texttt{tune()} function which performs by default a 10-fold Cross-Validation to determine the best tuning parameter $C$.
\begin{lstlisting}
library('e1071')
model.tune = tune(svm, y~., data= train_set, kernel= "linear", ranges= list(c(0.01, 0.1, 1, 10), gamma= c(0.1, 1, 10)))
summary(model.tune)
\end{lstlisting}

\begin{verbatim}
Parameter tuning of ‘svm’:
- sampling method: 10-fold cross validation 
- best parameters:
 Var1 gamma
 0.01   0.1
\end{verbatim}

The \texttt{tune()} function stores the best model that can be accessed as follows: \texttt{model.tune\$best.parameters} We can now fit the SVM model with those best tuning parameters.

The argument \texttt{scale=TRUE} tells the \textbf{svm()} function to scale each feature to have mean zero or standard deviation one. If the response has more than two levels, then the \texttt{svm()} function will perform multi-classification using the One-Versus-One approach.
\begin{lstlisting}
model.svm= svm(y~., data= train_set, kernel= 'linear', gamma= model.tune$best.parameters$gamma, cost= model.tune$best.parameters$Var1)
summary(model.svm)
\end{lstlisting}

\begin{verbatim}
Parameters:
   SVM-Type:  C-classification 
 SVM-Kernel:  linear 
       cost:  0.01 
      gamma:  0.1 
Number of Support Vectors:  124
 ( 14 22 22 20 21 25 )
Number of Classes:  6 
Levels: 
 1 2 3 4 5 6
\end{verbatim}
We see that the function identified 6 classes. This tells us, for instance, that a linear kernel was used with cost=0.01 , and that there were 124 support vectors, 14 in class 1, 22 in class 2, 22 in class 3, 20 in class 4, 21 in class 5 and 25 in class 6. 
We can now plot the support vector classifier obtained:
\begin{lstlisting}
plot(x= model.svm, data= svm.train, formula= X1~X2)
\end{lstlisting}
\begin{center}
	\includegraphics[width=0.8\linewidth]{Figures/svm_train_plot.png}
	\captionof{figure}{SVM on train set observations}
	\label{SVM on train set}
\end{center}
We note that here the second feature is plotted on the x-axis and the first feature is plotted on the y-axis, 

The region that is assigned to each class is shown on the right (light blue for 1 to dark purple for 6). The decision boundary between the two classes is linear, because we used the argument \texttt{kernel="linear"}. 
\\We then computer the errors and the confusion matrix.
\pagebreak
\begin{lstlisting}
confusion_matrix_svm.train = table(model.svm$fitted, train_set$y)
errors_svm.train = sum(model.svm$fitted != train_set$y)/length(train_set$y)
\end{lstlisting}
\begin{verbatim}
   1  2  3  4  5  6
  1 22  0  0  0  0  0
  2  0 20  0  0  0  0
  3  0  0 24  0  0  0
  4  0  0  0 27  0  0
  5  0  0  0  0 25  0
  6  0  0  0  0  0 26
0
\end{verbatim}
Our train error is null, no observation were miss-classified while training the model, which means that the linear boundary was enough. This is not really surprising as we have only 144 observations in the train set, but 3360 variables. Since $P \gg N$, it was easy for the model to define a hyperplane that fully separates the classes. 

However we are more interested in the support vector classifier’s performance on the test observations. We call the \texttt{predict} function that takes the SVM model as parameter.
\begin{lstlisting}
yhat= predict(model.svm, newdata= test_set)
confusion_matrix_svm.test= table(yhat, test_set$y)
errors_svm.test = sum(yhat!=test_set$y)/length(test_set$y)
plot(x= model.svm, data= test_set, formula= X1~X2)
\end{lstlisting}
\begin{verbatim}
 yhat  1  2  3  4  5  6
   1 12  0  0  0  0  1
   2  0 13  0  0  0  0
   3  0  0 10  0  1  2
   4  0  1  0  9  3  0
   5  0  2  2  0  7  0
   6  2  0  0  0  0  7
 0.1944444
\end{verbatim}

\begin{center}
	\includegraphics[width=0.8\linewidth]{Figures/svm_test_plot.png}
	\captionof{figure}{SVM on test set observations}
	\label{SVM on test set}
\end{center}

\pagebreak
\section{Conclusion}
Here is a summary of the error-rate we found for each method used so far : 
\begin{center}
\begin{tabular}{l c}
\textbf{Model} & \textbf{Percentage of error} \\
KNN & 26.8\% \\
LDA & 21\% \\
LDA + PCA & 18\% \\
NNET + FDA & 28\% \\
NNET + FDA + Regularization & 21\% \\
Random Forest + FDA & 16\% \\
SVM with Linear Kernel & 19.4\%
\end{tabular}
\end{center}

The best classifiers seem to be the Linear Discriminant Analysis applied on a subset of linear Principal Components, and the Random Forest applied on the FDA dataset. The performance of our models could have been better if we had more data to work with.

\end{document}
